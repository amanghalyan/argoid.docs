[argoid:children]
docker
zookeeper
redis
ssh_gateway
hdfs
yarn
nifi
prometheus
alert_manager
hive
mysql
airflow
alertmanager
grafana
presto
cassandra
kairosdb

[argoid:vars]

ssh_users=["chackra", "jeyaraj"]
delete_ssh_users=["chackra", "jeyaraj"]
#create_ssh_private_keys=False
enable_superuser_privileges=True
ssh_superusers=["chackra", "jeyaraj"]
selinux_state=disabled
list_disks_shell_command=" fdisk -l 2>/dev/null |awk '/^Disk \//{print substr($2,0,length($2)-1)}' | awk '!/sda/' | awk '!/sdb/' | sort "
disk_type=sdc
disk_mount_folder=/data/1
fs_file_max=262144
nofile_limit=262144
common_utilities=["epel-release", "lsof", "wget"]
argoid_nexus_ip=10.0.0.29
argoid_bigtop_ip=52.156.121.90
bigtop_url=http://{{argoid_bigtop_ip}}/repos
repo_name="argoid.repo"
jdk_download_url=http://{{argoid_bigtop_ip}}/argoid_repo/java/jdk-8u201-linux-x64.rpm
argoid_nexus_port=8081
argoid_bigtop_repo_url=http://{{argoid_bigtop_ip}}/repos
argoid_repository_hostname=argoid-demo-gw-1
argoid_nexus_repo_url=http://{{argoid_nexus_ip}}:{{argoid_nexus_port}}/repository/argoid-rpm-repos/rhel/x86-64/
java_home_path=/usr/java/latest/
jmx_exporter_prometheus_download_url=http://{{argoid_bigtop_ip}}/argoid_repo/jmx_prometheus_javaagent-0.3.0.jar
jmx_exporter_dir=/opt/jmx_exporter
jmx_exporter_config_file={{jmx_exporter_dir}}/config.yml
jmx_exporter_java_agent_file={{jmx_exporter_dir}}/jmx_prometheus_javaagent-0.3.0.jar
alert_manager_port=9093 #To be moved to alert_manager defaults once alert_manager role is created
docker_python_interpreter={{ docker_venv_path }}/bin/python
docker_venv_path=/opt/docker_venv

[ssh_gateway]
10.0.0.23
10.0.0.42

[ssh_gateway:vars]
create_ssh_private_keys=True
enable_superuser_privileges=True
ssh_superusers=["chackra", "jeyaraj"]

[docker]
10.0.0.38
10.0.0.39
10.0.0.40
10.0.0.33
10.0.0.44

[docker:vars]
docker_edition='ce'
docker_package="docker-{{ docker_edition }}"
docker_package_state=present
docker_yum_repo_url=https://download.docker.com/linux/{{ (ansible_distribution == "Fedora") | ternary("fedora","centos") }}/docker-{{ docker_edition }}.repo
docker_yum_gpg_key=https://download.docker.com/linux/centos/gpg
docker_compose_version="1.23.2"
docker_venv_path=/opt/docker_venv
docker_installation_python_path=/usr/bin/python
docker_python_interpreter={{ docker_venv_path }}/bin/python


[python3_venv]
10.0.0.44

[python3_venv:vars]
python_venv_version=3.6
python3_virtualenv_path=/opt/python3_venv


[zookeeper]
10.0.0.38
10.0.0.39
10.0.0.40

[zookeeper:vars]
#attached_disks=["sdc", "sdd"]
zookeeper_port=2181
jmx_prometheus_zookeeper_port=10801
zookeeper_initial_memory_allocation=256m
zookeeper_maximum_memory_allocation=256m


[redis:children]
redis_server
redis_sentinel

[redis_server]
10.0.0.38
10.0.0.39
10.0.0.40

[redis_sentinel]
10.0.0.38
10.0.0.39
10.0.0.40

[redis:vars]
redis_port=6379
redis_master_ip={{hostvars[inventory_hostname].groups['redis_server'][0]}}
redis_master_enabled={{ groups.redis_server | count > 1}}
redis_source_url=http://download.redis.io/releases/redis-6.2.0.tar.gz
redis_sentinel_port=16379
redis_sentinel_enabled=True
redis_version=redis-6.2.0
force=yes
redis_password=123
redis_master_password=123
redis_dir= /usr/local/src/redis
redis_db_dir=/var/lib/redis
redis_log_file=/var/log/redis/redis.log
sentinel_log_file=/var/log/redis/redis-sentinel.log
redis_exporter_docker_image=oliver006/redis_exporter
redis_exporter_container_name=redis-exporter
redis_exporter_port=9121
redis_data_source_name=redis://{{ansible_default_ipv4.address}}:{{redis_port}}
redis_replica_parallel_syncs=2

[hadoop_cluster:children]
yarn
hdfs

[hadoop_cluster:vars]
hdfs_bootstrap=false
# hdfs_cluster_name - hdfs-nameservice name
hdfs_cluster_name="ansiblecluster"
# hdfs_fs_trash_interval - trash retention time in minutes
hdfs_fs_trash_interval=4320
hdfs_dfs_replication=1
hdfs_dfs_datanode_du_reserved=1073741824
hdfs_dfs_journalnode_edits_dir=/data/1/dfs/jn
hdfs_namenode_dir=/data/1/dfs/nn
hdfs_datanode_dir_list=['/data/1/dfs/data', '/data/2/dfs/data']
hadoop_log_maxfilesize="256MB"
hadoop_log_maxbackupindex=20
hadoop_security_logger="INFO,RFAS"
hadoop_audit_logger="INFO,RFAAUDIT"
hdfs_audit_rotate_days=90
hdfs_java_home=/usr/java/jdk1.8.0_201-amd64/
hdfs_namenode_heap_size="1024m"
hdfs_namenode_javaOpts="-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=33701 -javaagent:{{jmx_exporter_java_agent_file}}=33301:{{jmx_exporter_config_file}} "
hdfs_datanode_javaOpts="-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=33601 -javaagent:{{jmx_exporter_java_agent_file}}=33303:{{jmx_exporter_config_file}}"

hdfs_namenodes={{ groups.hdfs_namenode }}
hdfs_ha_enabled={{ groups.hdfs_namenode | count > 1}}
hdfs_nameservices={{ hdfs_cluster_name }}
hdfs_default_fs="hdfs://{{ hdfs_nameservices if hdfs_ha_enabled else groups.hdfs_namenode[0] + ':8020' }}"
jobhistoryserver_enabled={{ groups.history_jobhistoryserver | count > 0}}
jobhistoryserver_address={{hostvars[inventory_hostname].groups['history_jobhistoryserver'][0]}}:10020
jobhistoryserver_webapp_address={{hostvars[inventory_hostname].groups['history_jobhistoryserver'][0]}}:19888

yarn_enable_log_aggregation=True
yarn_nodemanager_local_dirs_path=['/data/1/tmp/yarn/local']
yarn_nodemanager_log_dirs_path=/data/1/hadoop-yarn/containers
jobhistory_ipc_port=10020
yarn_timeline_web_port=8188
jobhistory_web_port=19888
yarn_nodemanager_remote_app_log_dir=/app-logs
yarn_nodemanager_log_retain_seconds=2000
YARN_RESOURCEMANAGER_HEAPSIZE=
yarn_resourcemanager_heapsize=512
yarn_nodemanager_heapsize=512
yarn_scheduler_maximum_allocation_mb=3000
yarn_nodemanager_resource_memory_mb=2000
yarn_nodemanager_resource_cpu_vcores=4
yarn_resourcemanager_zk_path=/yarn-ansible-zk
yarn_resourcemanager_cluster_id=yarnanisblecluster
yarn_resourcemanager_ha_automatic_failover_zk_base_path=/yarn-ansible-zk-ha
spark_source_url=http://52.156.121.90/argoid_repo/spark/spark-2.3.1-bin-hadoop2.7.tgz
spark_dir=/opt/spark
spark_history_fs_cleaner_maxAge=48h
spark_history_server_web_port=18080

yarn_resourcemanager_javaOpts="-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=23701 -javaagent:{{jmx_exporter_java_agent_file}}=23301:{{jmx_exporter_config_file}} "
yarn_nodemanager_javaOpts="-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=23705 -javaagent:{{jmx_exporter_java_agent_file}}=23305:{{jmx_exporter_config_file}} "

yarn_ha_enabled={{ groups.yarn_resourcemanager | count > 1}}
yarn_timelineserver_web_url={{hostvars[inventory_hostname].groups['yarn_timeline_server'][0]}}:{{yarn_timeline_web_port}}
yarn_log_server_url={{hostvars[inventory_hostname].groups['yarn_mapreduce_jobhistory_server'][0]}}:{{jobhistory_web_port}}
yarn_mapreduce_jobhistory_address={{hostvars[inventory_hostname].groups['yarn_mapreduce_jobhistory_server'][0]}}:{{jobhistory_ipc_port}}
spark_history_server_web_url={{hostvars[inventory_hostname].groups['spark_history_server'][0]}}:{{spark_history_server_web_port}}


[hdfs:children]
hdfs_namenode
hdfs_datanode
hdfs_journalnode

[hdfs_namenode]
10.0.0.38
10.0.0.39

[hdfs_datanode]
10.0.0.38
10.0.0.39
10.0.0.40

[hdfs_journalnode]
10.0.0.38
10.0.0.39
10.0.0.40

[yarn:children]
yarn_resourcemanager
yarn_nodemanager
yarn_timeline_server
yarn_mapreduce_jobhistory_server
spark_history_server
hadoop_clients

[yarn_resourcemanager]
10.0.0.38
10.0.0.39

[yarn_nodemanager]
10.0.0.40
10.0.0.39

[yarn_timeline_server]
10.0.0.38

[yarn_mapreduce_jobhistory_server]
10.0.0.38

[spark_history_server]
10.0.0.38

[hadoop_clients:children]
zeppelin
airflow
hadoop_client_ips

[hadoop_client_ips]
192.2.1.15
192.2.1.32

[hive:children]
hive_metastore
hive_client

[hive_metastore]
10.0.0.39


[hive_client]
10.0.0.23

[hive:vars]
hive_server2_port=10000
hive_metastore_port=9083
hive_heap_size=512
mysql_connector_source_url=http://52.156.121.90/argoid_repo/mysql/mysql-connector-java-8.0.15.jar


[postgres]
10.0.0.23

[postgres:vars]
postgres_mount_data_dir=/data/1/postgres_data
postgres_password=postgres
postgres_port=5432
postgres_container_name=postgres
postgres_docker_image=postgres:12
postgres_exporter_container_name=postgres-exporter
postgres_exporter_docker_image=wrouesnel/postgres_exporter
postgres_exporter_port=9187
postgres_data_source_name=postgresql://postgres:{{postgres_password}}@{{ansible_default_ipv4.address}}:{{postgres_port}}/?sslmode=disable

[nifi]
10.0.0.33

[nifi:vars]
nifi_dir=/opt/nifi
nifi_source_url==https://archive.apache.org/dist/nifi/1.13.2/nifi-1.13.2-bin.tar.gz
force=yes
nifi_port=8080
nifi_user=argoid
nifi_jmx_port=9341
nifi_java_heap_size=612

[node_exporter]
10.0.0.23

[node_exporter:vars]
node_exporter_source_url=https://github.com/prometheus/node_exporter/releases/download/v1.2.2/node_exporter-1.2.2.linux-amd64.tar.gz
node_exporter_dir= /opt/node_exporter
node_exporter_port=9100
force=yes
node_exporter_version=node_exporter-1.2.2.linux-amd64

[kafka]
10.0.0.38
10.0.0.39

[kafka:vars]
kafka_data_dir=/data/1/kafka
kafka_port=9092
kafka_jmx_port=39321
kafka_heap_size=1G

[prometheus]
#10.0.0.40
10.0.0.24

[prometheus:vars]
prometheus_project=prod_saas
prometheus_mount_config_dir=/opt/prometheus
prometheus_config_dir=/etc/prometheus
prometheus_port=9090
prometheus_container_name=prometheus
prometheus_docker_image=prom/prometheus:v2.22.1
node_exporter_port=9100
node_exporter_container_name=node_exporter
node_exporter_docker_image=prom/node-exporter:latest
prometheus_mount_data_dir=/data/1/prometheus
prometheus_data_dir=/prometheus

[prometheus_node]
10.0.0.40 prometheus_job_custom_config="{\"metrics_path\":\"/prometheus\",\"metric_relabel_configs\":[{\"action\":\"replace\",\"source_labels\":\"[exported_job]\",\"target_label\":\"jenkins_job\"},{\"action\":\"labeldrop\",\"regex\":\"exported_job\"}]}" # Temporary entry

[prometheus_node:vars]
prometheus_job_name_prefix = node_
prometheus_scrape_port = 9200


[alertmanager]
10.0.0.38

[alertmanager:vars]
alertmanager_port=9093
alert_recievers=alerts-saas-stage1
alertmanager_receiver_mail=alerts-saas-stage1@argoid.com
smtp_auth_username=devops-user1@argoid.com
smtp_auth_password=DevOps@2020

[grafana]
10.0.0.38

[nginx]
10.0.0.23

[nginx:vars]
nginx_port=8000

[redash]
10.0.0.41

[redash:vars]
redash_http_port=80
redash_docker_image_tag=8.0.0.b32245
redash_redis_docker_image_tag=5.0-alpine
redash_postgres_docker_image_tag=9.6-alpine
cadvisor_port=8089
redashdb_backup=False
az_storage_key=UilImJy48TPIgdwOCWXQAi9ovSEJCg5lqzJw24ZBra4VFYgOeo458HB+OdkLy43w0I9LEhD2PwlwpoOVovoRiA==
az_auth_mode=key
az_storage_account=argoidinfrabackups
az_storage_string=DefaultEndpointsProtocol=https;AccountName=argoidinfrabackups;AccountKey=UilImJy48TPIgdwOCWXQAi9ovSEJCg5lqzJw24ZBra4VFYgOeo458HB+OdkLy43w0I9LEhD2PwlwpoOVovoRiA==;EndpointSuffix=core.windows.net
ssl_enabling=True
nginx_port=443
redash_domain_name=ansible.redash.argoid.com
ssl_certificate=/etc/letsencrypt/live/{{ redash_domain_name }}/fullchain.pem
ssl_certificate_key=/etc/letsencrypt/live/{{ redash_domain_name }}/privkey.pem
ssl_trusted_certificate=/etc/letsencrypt/live/{{ redash_domain_name }}/chain.pem


[mysql]
10.0.0.23

[mysql:vars]
mysql_root_password=Password@123
mysql_exporter_port=9104
mysql_exporter_source_url=https://github.com/prometheus/mysqld_exporter/releases/download/v0.13.0/mysqld_exporter-0.13.0.linux-amd64.tar.gz
mysql_exporter_version=mysqld_exporter-0.13.0.linux-amd64
mysql_exporter_root_dir=/opt/mysql_exporter

[zeppelin]
10.0.0.40

[zeppelin:vars]
zeppelin_source_url=https://archive.apache.org/dist/zeppelin/{{zeppelin_version}}/{{ zeppelin_version }}-bin-all.tgz
zeppelin_dir=/opt/zeppelin
zeppelin_port=9081
force=yes
zeppelin_jmx_port=40023
zeppelin_version=zeppelin-0.8.1
zeppelin_jvm_heap_size=2048

[airflow]
10.0.0.45
10.0.0.41

[airflow:vars]
airflow_webserver_port=8082
airflow_enable_statsd_monitoring=True

[presto]
10.0.0.40

[presto:vars]
presto_max_heap_size=3G
query_max_memory=2GB
query_max_memory_per_node=1GB
query_max_total_memory_per_node=1GB
package_upgrade=no
force=yes

[cassandra]
10.0.0.40


[cassandra:vars]
cassandra_port=9042
force=yes
cassandra_jmx_port=13305
cassandra_max_heap_size=2G
cassandra_heap_newsize=200M

[kairosdb]
10.0.0.40

[kairosdb:vars]
argoid_repo=52.156.121.90/argoid_repo
