[argoid:children]
docker
zookeeper
redis
ssh_gateway
hdfs
yarn
nifi
prometheus
alert_manager
hive
mysql
airflow
alertmanager
grafana
presto
cassandra
kairosdb

[argoid:vars]

ssh_users=["chackra", "jeyaraj"]
delete_ssh_users=[]
#create_ssh_private_keys=False
enable_superuser_privileges=True
ssh_superusers=["chackra", "jeyaraj"]
selinux_state=disabled
list_disks_shell_command=" fdisk -l 2>/dev/null |awk '/^Disk \//{print substr($2,0,length($2)-1)}' | awk '!/sda/' | awk '!/sdb/' | sort "
disk_type=sdc
disk_mount_folder=/data/1
fs_file_max=262144
nofile_limit=262144
common_utilities=["epel-release", "lsof", "wget"]
argoid_nexus_ip=10.0.0.29
argoid_bigtop_ip=52.156.121.90
bigtop_url=http://{{argoid_bigtop_ip}}/repos
repo_name="argoid.repo"
jdk_download_url=http://{{argoid_bigtop_ip}}/argoid_repo/java/jdk-8u201-linux-x64.rpm
argoid_nexus_port=8081
argoid_bigtop_repo_url=http://{{argoid_bigtop_ip}}/repos
argoid_repository_hostname=argoid-demo-gw-1
argoid_nexus_repo_url=http://{{argoid_nexus_ip}}:{{argoid_nexus_port}}/repository/argoid-rpm-repos/rhel/x86-64/
java_home_path=/usr/java/latest/
jmx_exporter_prometheus_download_url=http://{{argoid_bigtop_ip}}/argoid_repo/jmx_prometheus_javaagent-0.3.0.jar
jmx_exporter_dir=/opt/jmx_exporter
jmx_exporter_config_file={{jmx_exporter_dir}}/config.yml
jmx_exporter_java_agent_file={{jmx_exporter_dir}}/jmx_prometheus_javaagent-0.3.0.jar
alert_manager_port=9093 #To be moved to alert_manager defaults once alert_manager role is created


[ssh_gateway]
192.0.1.4

[ssh_gateway:vars]
create_ssh_private_keys=True
enable_superuser_privileges=True
ssh_superusers=["chackra", "jeyaraj"]

[docker]
192.0.1.12
192.0.1.15
192.0.1.17
192.0.1.25
192.0.1.26
192.0.1.27
192.0.1.28


[docker:vars]
docker_edition='ce'
docker_package="docker-{{ docker_edition }}"
docker_package_state=present
docker_yum_repo_url=https://download.docker.com/linux/{{ (ansible_distribution == "Fedora") | ternary("fedora","centos") }}/docker-{{ docker_edition }}.repo
docker_yum_gpg_key=https://download.docker.com/linux/centos/gpg
docker_compose_version="1.23.2"
docker_venv_path=/opt/docker_venv
docker_installation_python_path=/usr/bin/python
docker_python_interpreter={{ docker_venv_path }}/bin/python


[python3_venv]
10.0.0.38
10.0.0.39
10.0.0.33

[python3_venv:vars]
python_venv_version=3.6
python3_virtualenv_path=/opt/python3_venv


[zookeeper]
192.0.1.5
192.0.1.6
192.0.1.7

[zookeeper:vars]
zookeeper_port=2181
jmx_prometheus_zookeeper_port=10801
zookeeper_initial_memory_allocation=256m
zookeeper_maximum_memory_allocation=256m


[redis:children]
redis_server
redis_sentinel

[redis_server]
192.0.1.17

[redis_sentinel]

[redis:vars]
redis_port=6379
redis_master_ip={{hostvars[inventory_hostname].groups['redis_server'][0]}}
#redis_master_enabled={{ groups.redis_server | count > 1}}
redis_master_enabled=False
redis_source_url=http://download.redis.io/releases/redis-6.2.0.tar.gz
redis_sentinel_port=16379
redis_sentinel_enabled=False
redis_version=redis-6.2.0
force=yes
redis_password=123
redis_master_password=123
redis_dir= /usr/local/src/redis
redis_db_dir=/var/lib/redis
redis_log_file=/var/log/redis/redis.log
sentinel_log_file=/var/log/redis/redis-sentinel.log
redis_exporter_docker_image=oliver006/redis_exporter
redis_exporter_container_name=redis-exporter
redis_exporter_port=9121
redis_data_source_name=redis://{{ansible_default_ipv4.address}}:{{redis_port}}
redis_replica_parallel_syncs=2


[haproxy]
192.0.1.17
[haproxy:vars]
#


[reco_rest]
192.0.1.17
[reco_rest:vars]
#


[reco_cache]
192.0.1.17
[reco_cache:vars]
#





[hadoop_cluster:children]
yarn
hdfs

[hadoop_cluster:vars]
hdfs_bootstrap=false
# hdfs_cluster_name - hdfs-nameservice name
hdfs_cluster_name="ansiblecluster"
# hdfs_fs_trash_interval - trash retention time in minutes
hdfs_fs_trash_interval=4320
hdfs_dfs_replication=1
hdfs_dfs_datanode_du_reserved=1073741824
hdfs_dfs_journalnode_edits_dir=/data/1/dfs/jn
hdfs_namenode_dir=/data/1/dfs/nn
hdfs_datanode_dir_list=['/data/1/dfs/data', '/data/2/dfs/data']
hadoop_log_maxfilesize="256MB"
hadoop_log_maxbackupindex=20
hadoop_security_logger="INFO,RFAS"
hadoop_audit_logger="INFO,RFAAUDIT"
hdfs_audit_rotate_days=90
hdfs_java_home=/usr/java/jdk1.8.0_201-amd64/
hdfs_namenode_heap_size="1024m"
hdfs_namenode_javaOpts="-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=33701 -javaagent:{{jmx_exporter_java_agent_file}}=33301:{{jmx_exporter_config_file}} "
hdfs_datanode_javaOpts="-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=33601 -javaagent:{{jmx_exporter_java_agent_file}}=33303:{{jmx_exporter_config_file}}"

hdfs_namenodes={{ groups.hdfs_namenode }}
hdfs_ha_enabled={{ groups.hdfs_namenode | count > 1}}
hdfs_nameservices={{ hdfs_cluster_name }}
hdfs_default_fs="hdfs://{{ hdfs_nameservices if hdfs_ha_enabled else hdfs_namenode[0] + ':8020' }}"
jobhistoryserver_enabled={{ groups.history_jobhistoryserver | count > 0}}
jobhistoryserver_address={{hostvars[inventory_hostname].groups['history_jobhistoryserver'][0]}}:10020
jobhistoryserver_webapp_address={{hostvars[inventory_hostname].groups['history_jobhistoryserver'][0]}}:19888

yarn_enable_log_aggregation=True
yarn_nodemanager_local_dirs_path=['/data/1/tmp/yarn/local']
yarn_nodemanager_log_dirs_path=/data/1/hadoop-yarn/containers
jobhistory_ipc_port=10020
yarn_timeline_web_port=8188
jobhistory_web_port=19888
yarn_nodemanager_remote_app_log_dir=/app-logs
yarn_nodemanager_log_retain_seconds=2000
yarn_resourcemanager_heapsize=512
yarn_nodemanager_heapsize=512
yarn_scheduler_maximum_allocation_mb=3000
yarn_nodemanager_resource_memory_mb=2000
yarn_nodemanager_resource_cpu_vcores=4
yarn_resourcemanager_zk_path=/yarn-ansible-zk
yarn_resourcemanager_cluster_id=yarnanisblecluster
yarn_resourcemanager_ha_automatic_failover_zk_base_path=/yarn-ansible-zk-ha
spark_source_url="http://{{ argoid_bigtop_ip }}/argoid_repo/spark/spark-2.3.1-bin-hadoop2.7.tgz"
spark_dir=/opt/spark
spark_history_fs_cleaner_maxAge=48h
spark_history_server_web_port=18080

yarn_resourcemanager_javaOpts="-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=23701 -javaagent:{{jmx_exporter_java_agent_file}}=23301:{{jmx_exporter_config_file}} "
yarn_nodemanager_javaOpts="-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=23705 -javaagent:{{jmx_exporter_java_agent_file}}=23305:{{jmx_exporter_config_file}} "

yarn_ha_enabled={{ groups.yarn_resourcemanager | count > 1}}
yarn_timelineserver_web_url={{hostvars[inventory_hostname].groups['yarn_timeline_server'][0]}}:{{yarn_timeline_web_port}}
yarn_log_server_url={{hostvars[inventory_hostname].groups['yarn_mapreduce_jobhistory_server'][0]}}:{{jobhistory_web_port}}
yarn_mapreduce_jobhistory_address={{hostvars[inventory_hostname].groups['yarn_mapreduce_jobhistory_server'][0]}}:{{jobhistory_ipc_port}}
spark_history_server_web_url={{hostvars[inventory_hostname].groups['spark_history_server'][0]}}:{{spark_history_server_web_port}}


[hdfs:children]
hdfs_namenode
hdfs_datanode
hdfs_journalnode

[hdfs_namenode]
10.0.0.38
10.0.0.39

[hdfs_datanode]
10.0.0.38
10.0.0.39
10.0.0.40

[hdfs_journalnode]
10.0.0.38
10.0.0.39
10.0.0.40

[yarn:children]
yarn_resourcemanager
yarn_nodemanager
yarn_timeline_server
yarn_mapreduce_jobhistory_server
spark_history_server
hadoop_clients

[yarn_resourcemanager]
10.0.0.38
10.0.0.39

[yarn_nodemanager]
10.0.0.40
10.0.0.39

[yarn_timeline_server]
10.0.0.38

[yarn_mapreduce_jobhistory_server]
10.0.0.38

[spark_history_server]
10.0.0.38

[hadoop_clients]
10.0.0.23

